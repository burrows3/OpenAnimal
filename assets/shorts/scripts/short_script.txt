Language models do not store sentences; they bet on the next word. Text is broken into tokens, which can be whole words, pieces, or punctuation. During training, the model reads huge text collections and learns patterns of what tends to follow what in many contexts. Given a prompt, it computes probabilities for the next token, selects one, and repeats, building a sequence that sounds fluent. Each new token updates the context, shifting the odds for what comes next. In many systems, thousands of possible next tokens are scored at every step. Follow for more.
