1
00:00:00,000 --> 00:00:04,456
Language models do not store sentences;
they bet on the next word.

2
00:00:04,456 --> 00:00:09,283
Text is broken into tokens, which can be
whole words, pieces, or punctuation.

3
00:00:09,283 --> 00:00:16,709
During training, the model reads huge text collections
and learns patterns of what tends to follow what in many contexts.

4
00:00:16,709 --> 00:00:24,135
Given a prompt, it computes probabilities for the next token,
selects one, and repeats, building a sequence that sounds fluent.

5
00:00:24,135 --> 00:00:28,962
Each new token updates the context,
shifting the odds for what comes next.

6
00:00:28,962 --> 00:00:33,789
In many systems, thousands of possible next tokens
are scored at every step.

7
00:00:33,789 --> 00:00:34,903
Follow for more.
